{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5m0W91xNK/pYOrRCvidJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabzim/4incubator/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6ngxIII9dwg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t2Fz-n819gHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading SafeSpacesNLP Project Data**"
      ],
      "metadata": {
        "id": "EmzipUhE9o9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install xlrd\n",
        "import xlrd\n",
        "#df = pd.read_excel('sample.xlsx')\n",
        "book = xlrd.open_workbook(\"/content/drive/MyDrive/SafespacesNLP/community_content_partial_conversations3.xlsx\")"
      ],
      "metadata": {
        "id": "9jzl7GYc9wV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with book as wb:\n",
        "    sh = wb.sheet_by_index(0)\n",
        "    with open('community_content_partial_conversations3.csv', 'w', newline=\"\") as csv_file:\n",
        "        col = csv.writer(csv_file)\n",
        "        for row in range(sh.nrows):\n",
        "            col.writerow(sh.row_values(row))\n",
        "train = pd.read_csv('community_content_partial_conversations.csv')\n",
        "print(train)"
      ],
      "metadata": {
        "id": "UCWtrmGW9z1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "display(train.head())\n",
        "print(len(train))"
      ],
      "metadata": {
        "id": "O86opL9H97BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Assessing Data Distribution ***"
      ],
      "metadata": {
        "id": "6edJx2t4FO8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train[\"event_type\"].value_counts()\n",
        "print(x)"
      ],
      "metadata": {
        "id": "IEph4EGEFQuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "sns.barplot(x.index, x)\n",
        "plt.gca().set_ylabel(\"samples\")\n",
        "plt.title(\"distribution\")"
      ],
      "metadata": {
        "id": "jfWrsw2vFWkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train[\"resource_type\"].value_counts()\n",
        "print(x)"
      ],
      "metadata": {
        "id": "O2lJFwl7FWqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "sns.barplot(x.index, x)\n",
        "plt.gca().set_ylabel(\"samples\")\n",
        "plt.title(\"distribution\")"
      ],
      "metadata": {
        "id": "ZebfbEp_Fcz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a series object containing the count of unique elements\n",
        "# in each column of dataframe\n",
        "uniqueValues = train.nunique()\n",
        "print('Count of unique value sin each column :')\n",
        "print(uniqueValues)"
      ],
      "metadata": {
        "id": "pogbcr1NFgFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[train[\"event_type\"] == 'Comment'][\"content\"].str.len())"
      ],
      "metadata": {
        "id": "qjFLdK0BFiye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.hist(train[train[\"event_type\"] == 'Comment'][\"content\"].str.len())\n",
        "plt.title(\"Comments length\")"
      ],
      "metadata": {
        "id": "cjKt4SboFlgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp=train[train[\"event_type\"] == 'Comment'][\"content\"].str.len()\n",
        "print(type(temp))\n",
        "print(temp.sum)\n",
        "Comments_Avg_length=temp.sum()/temp.count()\n",
        "print(\"Average length of comment posts is \", Comments_Avg_length )\n",
        "#print(Comments_Avg_length)"
      ],
      "metadata": {
        "id": "cJAPLirJFqFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "\n",
        "plt.hist(train[train[\"event_type\"] == 'Discussion'][\"content\"].str.len(), color= 'r')\n",
        "plt.title(\"Discussion length\")\n",
        "print(\"Average length of discussion posts is\")"
      ],
      "metadata": {
        "id": "_Ugxj0gkFuOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp=train[train[\"event_type\"] == 'Discussion'][\"content\"].str.len()\n",
        "print(type(temp))\n",
        "print(temp.sum)\n",
        "Discussions_Avg_length=temp.sum()/temp.count()\n",
        "print(\"Average length of discussion posts is \", Discussions_Avg_length )"
      ],
      "metadata": {
        "id": "8PpxQ7mWFw8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Corpus**"
      ],
      "metadata": {
        "id": "TtuurDNqFz1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus(target):\n",
        "     \n",
        "#target= 'Comment'\n",
        " corpus = []\n",
        " for x1 in train[train[\"event_type\"] == target][\"content\"].str.split():\n",
        "        #print(i)\n",
        "        #print(type(x1))\n",
        "        for i in x1:\n",
        "            print(i)   \n",
        "            corpus.append(i)\n",
        "            \n",
        " return corpus"
      ],
      "metadata": {
        "id": "mNFxeogZQtGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "ZgKGjSf2QzU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_type='Comment'\n",
        "corpus = create_corpus(event_type)"
      ],
      "metadata": {
        "id": "ZXP0JGKrQ2sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "dictionary = defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop:\n",
        "        dictionary[word] +=1\n",
        "        \n",
        "top = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "9xvsC7JBQ8H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = zip(*top)\n",
        "\n",
        "plt.grid()\n",
        "plt.bar(x,y, color = 'r')\n",
        "plt.title(\"top words in comment posts\")"
      ],
      "metadata": {
        "id": "vVQD4MvlRAVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_type='Discussion'\n",
        "corpus = create_corpus(event_type)"
      ],
      "metadata": {
        "id": "-pXuptuBRLJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "dictionary = defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop:\n",
        "        dictionary[word] +=1\n",
        "        \n",
        "top = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "KjHnUAkBRLQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = zip(*top)\n",
        "\n",
        "plt.grid()\n",
        "plt.bar(x,y, color = 'r')\n",
        "plt.title(\"top words of discussion posts\")"
      ],
      "metadata": {
        "id": "Bd1R0X6cRSjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuation**"
      ],
      "metadata": {
        "id": "c_MtKnuNRZa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_type='Discussion'\n",
        "\n",
        "corpus = create_corpus(event_type)\n",
        "dictionary = defaultdict(int)\n",
        "import string\n",
        "special_char = string.punctuation\n",
        "\n",
        "for i in corpus:\n",
        "    if i in special_char:\n",
        "        dictionary[i] +=1"
      ],
      "metadata": {
        "id": "DZBlyb0CRbtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = zip(*dictionary.items())\n",
        "\n",
        "plt.grid()\n",
        "plt.bar(x,y, color = 'r')\n",
        "plt.title(\"Punctuation disaster in Discussions\")"
      ],
      "metadata": {
        "id": "dwl7aK7GRgNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_type='Comment'\n",
        "\n",
        "corpus = create_corpus(event_type)\n",
        "dictionary = defaultdict(int)\n",
        "import string\n",
        "special_char = string.punctuation\n",
        "\n",
        "for i in corpus:\n",
        "    if i in special_char:\n",
        "        dictionary[i] +=1"
      ],
      "metadata": {
        "id": "2ltwVkegRjmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = zip(*dictionary.items())\n",
        "\n",
        "plt.grid()\n",
        "plt.bar(x,y, color = 'r')\n",
        "plt.title(\"Punctuation disaster in Comments\")"
      ],
      "metadata": {
        "id": "dXkIexB9Rm82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Words**"
      ],
      "metadata": {
        "id": "7HkiGyBGRswT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "GMYQsCjeRvY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(corpus)\n",
        "most = counter.most_common()\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for word, count in most[:40]:\n",
        "    if word not in stop:\n",
        "        x.append(word)\n",
        "        y.append(count)"
      ],
      "metadata": {
        "id": "19ywVGSFR0bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"most common words\")\n",
        "plt.grid()\n",
        "sns.barplot(x = y, y = x)"
      ],
      "metadata": {
        "id": "AEXMtZBER3xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data cleaning**"
      ],
      "metadata": {
        "id": "OHm9Jb8VR9Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.concat([train, test])\n",
        "#df.shape\n",
        "train.size\n",
        "df=train"
      ],
      "metadata": {
        "id": "UeS0yV9USBuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing URLs**"
      ],
      "metadata": {
        "id": "AMiUwu49SGg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)"
      ],
      "metadata": {
        "id": "WizYyQ9PSIKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"content\"].apply(lambda x: remove_url(x))"
      ],
      "metadata": {
        "id": "Xm1EQBOVSOJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove HTML tag**"
      ],
      "metadata": {
        "id": "iZ4RGPmvSRP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)"
      ],
      "metadata": {
        "id": "3U2XIKFNSVsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"content\"].apply(lambda x: remove_html(x))"
      ],
      "metadata": {
        "id": "2ltFNILRSaDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "JBx5je8lSbKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Emoji**"
      ],
      "metadata": {
        "id": "S2ykdm_VSjCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n",
        "                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"    \n",
        "                               \"]+\", flags = re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "ezQOS22vSog2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"content\"].apply(lambda x: remove_emoji(x))"
      ],
      "metadata": {
        "id": "LU4ODZe6SrpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VfT8MptmSuPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove punctuation**"
      ],
      "metadata": {
        "id": "kzB4tAy6SzZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)"
      ],
      "metadata": {
        "id": "-PPElVhES3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"content\"].apply(lambda x: remove_punctuation(x))"
      ],
      "metadata": {
        "id": "Rs0GevG2S8aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "iHLnnZIjTCci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spelling checker**"
      ],
      "metadata": {
        "id": "y9JX5mIKTFl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "EJI01fcMTHLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    \n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)"
      ],
      "metadata": {
        "id": "wRk9VztRTL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"]=df[\"content\"].apply(lambda x : correct_spellings(x))"
      ],
      "metadata": {
        "id": "liMb_94uTPTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glove vectorization (word2vec)**"
      ],
      "metadata": {
        "id": "Cx10UM8uTTIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "XJIGlS7BTWrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus(df):\n",
        "    corpus = []\n",
        "    for tweet in tqdm(df[\"content\"]):\n",
        "        words = [word.lower() for word in word_tokenize(tweet) if \\\n",
        "        ((word.isalpha() == 1) & (word not in stop))]\n",
        "        corpus.append(words)\n",
        "        \n",
        "    return corpus"
      ],
      "metadata": {
        "id": "H30GvkD8TZ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = create_corpus(df)"
      ],
      "metadata": {
        "id": "npVw2nwsTckW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = {}\n",
        "\n",
        "with open('/content/drive/MyDrive/SafespacesNLP/glove.twitter.27B.200d.txt','r') as glove:\n",
        "    for line in glove:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vectors = np.asarray(values[1:], 'float32')\n",
        "        embedding_dict[word] = vectors\n",
        "        \n",
        "glove.close()"
      ],
      "metadata": {
        "id": "NBGPj7SHTdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "kSuXhNn_TjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 50  #50 before\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(corpus)\n",
        "\n",
        "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "tweet_pad = pad_sequences(sequences,\n",
        "                          maxlen = MAX_LEN, \n",
        "                         truncating = 'post', \n",
        "                         padding = 'post')"
      ],
      "metadata": {
        "id": "e78T_-UWTm6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer_obj.word_index\n",
        "print('number of unique words: ', len(word_index))"
      ],
      "metadata": {
        "id": "NPXWtDeCTqpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words,200))\n",
        "\n",
        "\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "        \n",
        "    embedding_vector = embedding_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "K1TwaA7xTthr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(embedding_matrix))"
      ],
      "metadata": {
        "id": "DQfURsaGTwbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert numpy array to dictionary\n",
        "emmbed_dict = dict(enumerate(embedding_matrix.flatten(), 1))\n",
        "print(type(emmbed_dict))\n",
        "print(emmbed_dict)"
      ],
      "metadata": {
        "id": "siE34Z91Ty9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install distfit"
      ],
      "metadata": {
        "id": "6u3t9jUIT2Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vectors))"
      ],
      "metadata": {
        "id": "o2m5_3zeT4es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectors)"
      ],
      "metadata": {
        "id": "j0km3IP3T6y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling the Glove Vectors Through TSNE**"
      ],
      "metadata": {
        "id": "RaPtWLqzT9Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from distfit import distfit\n",
        "\n",
        "# Generate 10000 normal distribution samples with mean 0, std dev of 3 \n",
        "X = np.random.normal(0, 3, 10000)\n",
        "\n",
        "# Initialize distfit\n",
        "dist = distfit()\n",
        "\n",
        "# Determine best-fitting probability distribution for data\n",
        "dist.fit_transform(X)"
      ],
      "metadata": {
        "id": "oWDNknAEUCq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from distfit import distfit\n",
        "import numpy as np\n",
        "\n",
        "distri = TSNE(n_components=2)\n",
        "words = list(emmbed_dict.keys())\n",
        "vectors = [emmbed_dict[word] for word in words]\n",
        "\n",
        "# Initialize\n",
        "dist = distfit(todf=True)\n",
        "X=np.array(vectors)\n",
        "# Search for best theoretical fit on your empirical data\n",
        "y = dist.fit_transform(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "#plt.scatter(y[:, 0],y[:,1])\n",
        "#plt.scatter(y['model'],y['summary'])\n",
        "\n",
        "\n",
        "#for label,x,y in zip(words,y[:, 0],y[:,1]):\n",
        "#  plt.annotate(label,xy=(x,y),xytext=(0,0),textcoords='offset points')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "gXSZqMsYUIGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X))\n",
        "print(type(y))\n",
        "#print(y.keys())\n",
        "#print(y['summary'])"
      ],
      "metadata": {
        "id": "_r61XTs9UL2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing tsne code\n",
        "\n",
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "\n",
        "def display_closestwords_tsnescatterplot(model, dim, words):\n",
        "    \n",
        "    arr = np.empty((0,dim), dtype='f')\n",
        "    word_labels = words\n",
        "\n",
        "    # get close words\n",
        "    #close_words = [model.similar_by_word(word) for word in words]\n",
        "    \n",
        "    # add the vector for each of the closest words to the array\n",
        "    close_words=[]\n",
        "    for word in words:\n",
        "        arr = np.append(arr, np.array([model[word]]), axis=0)\n",
        "        close_words +=model.similar_by_word(word)\n",
        "        \n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model[wrd_score[0]]\n",
        "        word_labels.append(wrd_score[0])\n",
        "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
        "        \n",
        "    # find tsne coords for 2 dimensions\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    #np.set_printoptions(suppress=True)\n",
        "    Y = tsne.fit_transform(arr)\n",
        "\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "    # display scatter plot\n",
        "    plt.scatter(x_coords, y_coords)\n",
        "\n",
        "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
        "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
        "    plt.show()\n",
        "    \n",
        "def tsne_plot(model, words):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    #for word in model.wv.vocab:\n",
        "    for word in words:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(14, 10)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xRxFe0rRUUDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running LSTM on Safespaces NLP project**"
      ],
      "metadata": {
        "id": "DA86TGBeUZEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "glove_embedding = Embedding(num_words, 200, embeddings_initializer = Constant(embedding_matrix), \n",
        "                     input_length = MAX_LEN, \n",
        "                     trainable = False)\n",
        "\n",
        "model.add(glove_embedding)\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "model.add(Dense(128, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model.add(Dense(256, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-5)\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "HToC27Q6UeYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "e-S8H1-bUis5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}