{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first column is the review content (quoted)\n",
    "# second column is the assigned sentiment (positive or negative)\n",
    "def load_file():\n",
    "    with open('D:\\MS research\\submissions\\code\\data/labelled_tweets_balanced.csv',encoding=\"utf8\") as csv_file:\n",
    "        reader = csv.reader(csv_file,delimiter=\",\",quotechar='\"')\n",
    "        next(reader)\n",
    "        data =[]\n",
    "        target = []\n",
    "        for row in reader:\n",
    "            # skip missing data\n",
    "            if row[0] and row[1]:\n",
    "                data.append(row[0])\n",
    "                target.append(row[1])\n",
    "\n",
    "        return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, target = load_file()\n",
    "print(data[0],target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess_test(data):\n",
    "    dat = data\n",
    "    count_vectorizer = CountVectorizer(min_df=1,binary='false',ngram_range=(1,2))\n",
    "    dat = count_vectorizer.fit_transform(dat)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).transform(dat)\n",
    "\n",
    "    return tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess_tfidf(n):\n",
    "    data,target = load_file()\n",
    "    count_vectorizer = CountVectorizer(max_df=0.8,binary='true',ngram_range=(1,n))#,analyzer =text_process)\n",
    "    data = count_vectorizer.fit_transform(data)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\n",
    "\n",
    "    return tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess_bow(n):\n",
    "    dat,target = load_file()\n",
    "    count_vectorizer = CountVectorizer(max_df=0.8,binary='false',ngram_range=(1,n))\n",
    "    dat = count_vectorizer.fit_transform(dat)\n",
    "    #tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\n",
    "\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_model(data,target,classifier):\n",
    "    data_train, target_train = data, target\n",
    "    predicted = cross_val_predict(classifier,data_train, target_train, cv=10)\n",
    "    #print(classification_report(target,predicted))\n",
    "    evaluate_model(target_train,predicted)\n",
    "    scores = cross_val_score(classifier,data_train,target_train,cv=10)\n",
    "    print(\"10 Fold Cross Validation Scores for training data: \")\n",
    "    print(scores)\n",
    "    print()\n",
    "    print(\"The mean score and the 95% confidence interval of the score estimate are given by:\")\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" %(scores.mean(), scores.std()*2))\n",
    "    print()\n",
    "    #target = label_binarize(target, classes=[0, 1, 2])\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    target_train = np.array([number[0] for number in lb.fit_transform(target_train)])\n",
    "    recall = cross_val_score(classifier,data_train,target_train,cv=10, scoring='recall')\n",
    "    print(\"10 Fold Cross Validation Recall: \")\n",
    "    print(recall)\n",
    "    print()\n",
    "    #print(\"The mean score and the 95% confidence interval of the score estimate are given by:\")\n",
    "    print(\"Recall: %0.2f (+/- %0.2f)\" %(recall.mean(), recall.std()*2))\n",
    "    print()\n",
    "    precision = cross_val_score(classifier,data_train,target_train,cv=10, scoring='precision')\n",
    "    print(\"10 Fold Cross Validation Precision: \")\n",
    "    print(precision)\n",
    "    print()\n",
    "    #print(\"The mean score and the 95% confidence interval of the score estimate are given by:\")\n",
    "    print(\"Precision: %0.2f (+/- %0.2f)\" %(precision.mean(), precision.std()*2))\n",
    "    print()\n",
    "    \n",
    "    #evaluate_model(target,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(target_true,target_predicted):\n",
    "    class_names=['1','2','3']\n",
    "    print(classification_report(target_true,target_predicted))\n",
    "    cnf_matrix = confusion_matrix(target_true, target_predicted)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix')\n",
    "    plt.show()\n",
    "    #print('\\nConfussion matrix:\\n',confusion_matrix(target_true, target_predicted))\n",
    "    print()\n",
    "    print(\"The accuracy score is {:.2%}\".format(accuracy_score(target_true,target_predicted)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  Logistic():\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression unigram tf idf:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression unigram tf idf\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression unigram Term occurance:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression unigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression bigram tf idf:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression bigram tf idf\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression bigram Term occurance:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression bigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression unigram+bigram tf idf:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression unigram+bigram tf idf\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of logistic regression unigram+bigram Term occurance:\")\n",
    "    LogisticRegression_classifier = LogisticRegression()\n",
    "    print()\n",
    "    learn_model(data_train,target_train,LogisticRegression_classifier)\n",
    "    print()\n",
    "    print(\"Testing results of logistic regression unigram+bigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = LogisticRegression_classifier.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  KNN():\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 unigram tf idf:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 unigram tf-idf:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 unigram Term occurance:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 unigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\") \n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 bigram tf idf:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 bigram tf-idf:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 bigram Term occurance:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 bigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\") \n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 unigram+bigram tf idf:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 unigram+bigram tf-idf:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of KNN k=1 unigram+bigram Term occurance:\")\n",
    "    KNN_classifier1K = KNeighborsClassifier(n_neighbors=1)                       \n",
    "    print()\n",
    "    learn_model(data_train,target_train,KNN_classifier1K)\n",
    "    print()\n",
    "    print(\"Testing results of KNN k=1 unigram+bigram Term occurance:\")\n",
    "    print()\n",
    "    predicted = KNN_classifier1K.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\") \n",
    "KNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  Logistic():\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of unigram tf idf LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    classifier_l2_LR = LogisticRegression(C=100, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results of unigram tf idf LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results unigram Term occurance LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    classifier_l2_LR = LogisticRegression(C=1, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results unigram Term occurance LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of bigram tf idf with LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    classifier_l2_LR = LogisticRegression(C=100, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results of bigram tf idf LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of bigram term occurance with LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    classifier_l2_LR = LogisticRegression(C=1, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results of bigram term occurance with LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of unigram+bigram tf idf LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    classifier_l2_LR = LogisticRegression(C=100, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results of unigram+bigram  tf idf LogisticRegression with L2 penalty, Regularization Parameter C:100\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results unigram+bigram  Term occurance LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    classifier_l2_LR = LogisticRegression(C=1, penalty='l2', n_jobs=-1)\n",
    "    print()\n",
    "    learn_model(data_train,target_train,classifier_l2_LR)\n",
    "    print()\n",
    "    print(\"Testing results unigram+bigram  Term occurance LogisticRegression with L2 penalty, Regularization Parameter C:1\")\n",
    "    print()\n",
    "    predicted = classifier_l2_LR.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVC_Linear():\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear unigram tf idf C=1:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear unigram tf-idf  C=1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear unigram Term occurance  C=0.1:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=0.1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear unigram Term occurance  C=0.1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear bigram tf idf  C=1:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear bigram tf-idf  C=1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear bigram Term occurance C=0.1:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=0.1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear bigram Term occurance C=0.1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\") \n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear unigram+bigram tf idf  C=10:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear unigram+bigram tf-idf  C=1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of SVM linear unigram+bigram Term occurance C=0.1:\")\n",
    "    SVC_classifier_linearKernel = SVC(C=0.1, kernel='linear')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_linearKernel)\n",
    "    print()\n",
    "    print(\"Testing results of SVM linear unigram+bigram Term occurance C=0.1:\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_linearKernel.fit(data_train,target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\") \n",
    "SVC_Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVC_RBF():\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of unigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=1, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of unigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of unigram term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=0.01, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of unigram  term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of bigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=1, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of bigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of bigram term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=0.01, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of bigram  term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    data,target = load_file()\n",
    "    tfidf_data = preprocess_tfidf(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(tfidf_data, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of unigram+bigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=1, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of unigram+bigram tf idf SVM RBF kernel with C=10, gamma=1\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    bow=preprocess_bow(1+2)\n",
    "    data_train, data_test, target_train, target_test = train_test_split(bow, target, test_size=0.2, random_state=42)\n",
    "    print(\"Training results of bigram term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    SVC_classifier_rbfKernel = SVC(C=10,gamma=0.01, kernel='rbf')\n",
    "    print()\n",
    "    learn_model(data_train,target_train,SVC_classifier_rbfKernel)\n",
    "    print()\n",
    "    print(\"Testing results of bigram  term occurance SVM RBF kernel with C=10, gamma=0.01\")\n",
    "    print()\n",
    "    predicted = SVC_classifier_rbfKernel.fit(data_train, target_train).predict(data_test)\n",
    "    #print(classification_report(target_test,predicted))\n",
    "    evaluate_model(target_test,predicted)\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "SVC_RBF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
